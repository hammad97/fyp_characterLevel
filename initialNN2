#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Sep  4 00:50:06 2018

@author: hammad
"""

# Implementing TF-IDF
#---------------------------------------
#
# Here we implement TF-IDF,
#  (Text Frequency - Inverse Document Frequency)
#  for the spam-ham text data.
#
# We will use a hybrid approach of encoding the texts
#  with sci-kit learn's TFIDF vectorizer.  Then we will
#  use the regular TensorFlow logistic algorithm outline.

import tensorflow as tf
import matplotlib.pyplot as plt
import csv
import numpy as np
import os
import string
import requests
import io
import re
import time
import nltk
from zipfile import ZipFile
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.python.framework import ops
ops.reset_default_graph()

# Start a graph session
sess = tf.Session()

batch_size = 200
max_features = 1000

start_time=  time.time() 

                                                       #Calculating initial time
#filedata = []   
#
#
#for dirpath, dirnames, filenames in os.walk('Doc50'):                           #Reading all files in Doc50 folder and storing it names in
#    print(filenames[4])                                                         #filenames and path in dirpath
#    
#i=0
#
#
#for file in filenames:
#    filedata2= ''
#    fpath = os.getcwd()
#    fpath = os.path.join(fpath, str(dirpath))
#    fpath = os.path.join(fpath, str(filenames[i]))                              #fpath now contains whole path to each file at each iteration
#    filee = open(fpath,mode='r')
#    filedata2=filee.read()                                                      #here file is read and stored in a temporary variable
#    filedata2 = str.lower(filedata2)                                            
#    filedata2=''.join(e for e in filedata2 if e.isalpha() or e==' ')            #here file is normalized from noisy text data by only keeping alphabetical values
#    filedata2=re.sub(' +',' ',filedata2)                                        #after performing above step there is a increase in spaces to remove them this step is done
#    filedata.append(filedata2)                                                  #finally file data is moved to actual variable from temporary one
#    filee.close()
#    i=i+1
#
#unique = []
#
#for fdata in filedata:                                                          #each files data was stored in one single string therefore each string is split ...
#    neww=fdata.split(' ')                                                       #...to obtain all the words from that document to be later used in VSM
#    neww=neww[:-1]                                                              #each document's last index contains ' ' as a feature which is removed from every where
#    unique.append(neww)                                                         #IMPORTANT NOTE: unique doesnot contain unique words of all files it is just the variable name...                    
#                                                                                #...containing 2d list where each row contains list of words used in one file
categories = [
    'alt.atheism',
 'comp.graphics',
 'misc.forsale',
 'rec.autos'
 ]

dataset = fetch_20newsgroups(subset='all', categories=categories,
                             shuffle=True, random_state=42)

print("%d documents" % len(dataset.data))
print("%d categories" % len(dataset.target_names))
print()

labels = dataset.target
true_k = np.unique(labels).shape[0]


# Check if data was downloaded, otherwise download it and save for future use
#save_file_name = 'temp_spam_data.csv'
#if os.path.isfile(save_file_name):
#    text_data = []
#    with open(save_file_name, 'r') as temp_output_file:
#        reader = csv.reader(temp_output_file)
#        for row in reader:
#            text_data.append(row)
#else:
#    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'
#    r = requests.get(zip_url)
#    z = ZipFile(io.BytesIO(r.content))
#    file = z.read('SMSSpamCollection')
#    # Format Data
#    text_data = file.decode()
#    text_data = text_data.encode('ascii', errors='ignore')
#    text_data = text_data.decode().split('\n')
#    text_data = [x.split('\t') for x in text_data if len(x) >= 1]
#    
#    # And write to csv
#    with open(save_file_name, 'w') as temp_output_file:
#        writer = csv.writer(temp_output_file)
#        writer.writerows(text_data)


texts = dataset.data
target = dataset.target

# Relabel 'spam' as 1, 'ham' as 0
#target = [1. if x == 'spam' else 0. for x in target]

# Normalize text
# Lower case
texts = [x.lower() for x in texts]

# Remove punctuation
texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]

# Remove numbers
texts = [''.join(c for c in x if c not in '0123456789') for x in texts]

# Trim extra whitespace
texts = [' '.join(x.split()) for x in texts]


# Define tokenizer
def tokenizer(text):
    words = nltk.word_tokenize(text)
    return words

# Create TF-IDF of texts
tfidf = TfidfVectorizer(tokenizer=tokenizer, stop_words='english', max_features=max_features, max_df = 0.2)
sparse_tfidf_texts = tfidf.fit_transform(texts)

# Split up data set into train/test
print("SHAPE: ",sparse_tfidf_texts.shape[0])
train_indices = np.random.choice(sparse_tfidf_texts.shape[0], round(0.2*sparse_tfidf_texts.shape[0]), replace=False)
test_indices = np.array(list(set(range(sparse_tfidf_texts.shape[0])) - set(train_indices)))
texts_train = sparse_tfidf_texts[train_indices]
texts_test = sparse_tfidf_texts[test_indices]
target_train = np.array([x for ix, x in enumerate(target) if ix in train_indices])
target_test = np.array([x for ix, x in enumerate(target) if ix in test_indices])

# Create variables for logistic regression
A = tf.Variable(tf.random_normal(shape=[max_features, 1]))
b = tf.Variable(tf.random_normal(shape=[1, 1]))

# Initialize placeholders
x_data = tf.placeholder(shape=[None, max_features], dtype=tf.int32)
y_target = tf.placeholder(shape=[None, 1], dtype=tf.int32)

# Declare logistic model (sigmoid in loss function)
model_output = tf.add(tf.matmul(x_data, A), b)

# Declare loss function (Cross Entropy loss)
loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))

# Actual Prediction
prediction = tf.round(tf.sigmoid(model_output))
predictions_correct = tf.cast(tf.equal(prediction, y_target), tf.int32)
accuracy = tf.reduce_mean(predictions_correct)

# Declare optimizer
my_opt = tf.train.GradientDescentOptimizer(0.0025)
train_step = my_opt.minimize(loss)

# Intitialize Variables
init = tf.global_variables_initializer()
sess.run(init)

# Start Logistic Regression
train_loss = []
test_loss = []
train_acc = []
test_acc = []
i_data = []
for i in range(10000):
    rand_index = np.random.choice(texts_train.shape[0], size=batch_size)
    rand_x = texts_train[rand_index].todense()
    rand_y = np.transpose([target_train[rand_index]])
    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})
    
    # Only record loss and accuracy every 100 generations
    if (i + 1) % 100 == 0:
        i_data.append(i+1)
        train_loss_temp = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})
        train_loss.append(train_loss_temp)
        
        test_loss_temp = sess.run(loss, feed_dict={x_data: texts_test.todense(), y_target: np.transpose([target_test])})
        test_loss.append(test_loss_temp)
        
        train_acc_temp = sess.run(accuracy, feed_dict={x_data: rand_x, y_target: rand_y})
        train_acc.append(train_acc_temp)
    
        test_acc_temp = sess.run(accuracy, feed_dict={x_data: texts_test.todense(), y_target: np.transpose([target_test])})
        test_acc.append(test_acc_temp)
    if (i + 1) % 500 == 0:
        acc_and_loss = [i+1, train_loss_temp, test_loss_temp, train_acc_temp, test_acc_temp]
        acc_and_loss = [np.round(x,2) for x in acc_and_loss]
        print('Generation # {}. Train Loss (Test Loss): {:.2f} ({:.2f}). Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))


# Plot loss over time
plt.plot(i_data, train_loss, 'k-', label='Train Loss')
plt.plot(i_data, test_loss, 'r--', label='Test Loss', linewidth=4)
plt.title('Cross Entropy Loss per Generation')
plt.xlabel('Generation')
plt.ylabel('Cross Entropy Loss')
plt.legend(loc='upper right')
plt.show()

# Plot train and test accuracy
plt.plot(i_data, train_acc, 'k-', label='Train Set Accuracy')
plt.plot(i_data, test_acc, 'r--', label='Test Set Accuracy', linewidth=4)
plt.title('Train and Test Accuracy')
plt.xlabel('Generation')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()